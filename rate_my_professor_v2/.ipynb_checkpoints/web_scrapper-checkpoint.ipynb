{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a1508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f04d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Character from the text\n",
    "def Remove_Character(Text, characters):\n",
    "    val = Text.get_text(separator=' ').rstrip().split()\n",
    "    val = ' '.join(val)\n",
    "    for c in characters:\n",
    "        val = ''.join(val.split(c))\n",
    "    return val\n",
    "\n",
    "# Get the position value\n",
    "def Get_Position_Value(Text_Value, value_address):\n",
    "    val = Text_Value.get_text(separator=' ').rstrip().split()\n",
    "    return(val[value_address])\n",
    "\n",
    "# Join the array with given tag\n",
    "def Join_Value_With(value, join_value):\n",
    "    val = value.get_text(separator=' ').rstrip().split()\n",
    "    val = join_value.join(val)\n",
    "    return(val)\n",
    "\n",
    "# To Reamove Unusal Spaces\n",
    "def Remove_Unusal_Spaces(value):\n",
    "    try:\n",
    "        val = value.get_text(separator=' ').rstrip().split()\n",
    "        val = ' '.join(val)\n",
    "    except:\n",
    "        val = ''\n",
    "    return(val)\n",
    "\n",
    "# Make the attributes naming \n",
    "def Name_Attribute(attribute):\n",
    "    \"\"\"remove the unusual spaces and convert the sentence into array\"\"\"\n",
    "    attr = attribute.get_text(separator=' ').rstrip().split()\n",
    "    attr = [x.casefold() for x in attr] # capitalize the first letter of the word\n",
    "    attr = '_'.join(attr) # Join each word with \"_\"\n",
    "    return(attr)\n",
    "\n",
    "def Get_href(anchor):\n",
    "    \"\"\"scrap the anchor tag href\"\"\"\n",
    "    href =  anchor.find('a', href=True)['href']\n",
    "    return href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efae189",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(\"C:\\Program Files (x86)\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6fac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the url\n",
    "def get_url(url):\n",
    "    driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388d5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click cookies close btn\n",
    "def close_cookies():\n",
    "    cookies = driver.find_element(By.CLASS_NAME, 'CCPAModal__StyledCloseButton-sc-10x9kq-2')\n",
    "    cookies.click()\n",
    "    \n",
    "# click on close btn to close the Ad\n",
    "def close_Ad():\n",
    "    close_ad = driver.find_element(By.ID, 'bx-close-inside-1177612')\n",
    "    close_ad.click()\n",
    "    \n",
    "# Try to close cookies or ad    \n",
    "def close_cookies_or_ad():\n",
    "    try:\n",
    "        close_cookies()\n",
    "    except:\n",
    "        try:\n",
    "            close_Ad()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4a605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on show more btn\n",
    "def show_more_btn():\n",
    "    close_cookies_or_ad()\n",
    "    show_more = driver.find_element(By.CLASS_NAME, 'Buttons__Button-sc-19xdot-1')\n",
    "    show_more.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a4ea9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bs4_object(html_page):\n",
    "    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848968a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the current url html code\n",
    "def get_page_source():\n",
    "    close_cookies_or_ad()\n",
    "    result = driver.page_source\n",
    "    soup = to_bs4_object(result)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46dd663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_professor_count():\n",
    "    close_cookies_or_ad()\n",
    "    soup = get_page_source()\n",
    "    pagination_header = soup.find('div', class_='SearchResultsPage__SearchResultsPageHeader-vhbycj-3')\n",
    "    try:\n",
    "        professor_count = int(Get_Position_Value(pagination_header, 0))\n",
    "        uni_name = pagination_header.div.h1.span.b.get_text(separator=' ')\n",
    "    except:\n",
    "        professor_count = 0\n",
    "        uni_name = pagination_header.div.h1.span.b.get_text(separator=' ')\n",
    "    return (professor_count, uni_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322e4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper(page, uni_name, uid):\n",
    "    professor_data = []\n",
    "    professor_cards = page.find_all('a', class_='TeacherCard__StyledTeacherCard-syjs0d-0 dLJIlx')\n",
    "    for card in professor_cards:\n",
    "        u_name = card.find('div', class_='CardSchool__School-sc-19lmz2k-1').text\n",
    "        if u_name == uni_name:\n",
    "            pro_data = {}\n",
    "            pro_data['overall_rating'] = float(card.find('div', class_='CardNumRating__CardNumRatingNumber-sc-17t4b9u-2').text)\n",
    "            rating = card.find('div', class_='CardNumRating__CardNumRatingCount-sc-17t4b9u-3')\n",
    "            pro_data['ratings'] = int(Get_Position_Value(rating, 0))\n",
    "            pro_data['professor_name'] = card.find('div', class_='CardName__StyledCardName-sc-1gyrgim-0').text\n",
    "            pro_data['subject'] = card.find('div', class_='CardSchool__Department-sc-19lmz2k-0').text\n",
    "            card_feed = card.find_all('div', class_='CardFeedback__CardFeedbackNumber-lq6nix-2')\n",
    "            pro_data['level_of_difficulty'] = card_feed[1].text\n",
    "            pro_data['would_take_again'] = card_feed[0].text\n",
    "            professor_data.append(pro_data)\n",
    "    whole_data = {\n",
    "        'uni_id': uid,\n",
    "        'university_name': uni_name,\n",
    "        'professors': professor_data,\n",
    "    }\n",
    "    return whole_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd697e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Configuration\n",
    "user = 'web_scrapper'\n",
    "password = 'CW8Cty2cc9khaTdy'\n",
    "database = 'rate_my_professor_v2'\n",
    "connection_str = \"mongodb+srv://\" + user + \":\" + password + \"@cluster0.c4cev.mongodb.net/\" + database + \"?retryWrites=true&w=majority\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01458fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Status:\n",
      " Collection(Database(MongoClient(host=['cluster0-shard-00-00.c4cev.mongodb.net:27017', 'cluster0-shard-00-02.c4cev.mongodb.net:27017', 'cluster0-shard-00-01.c4cev.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', authsource='admin', replicaset='atlas-r06dpt-shard-0', tls=True), 'rate_my_professor_v2'), 'rate_my_professor.rate_my_professor_v2')\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(connection_str)\n",
    "db = client.get_database(database)\n",
    "college_db = db.rate_my_professor.rate_my_professor_v2\n",
    "print('Connection Status:\\n',college_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a0e5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = 6049\n",
    "start = 0\n",
    "status_count = db['status_lalit'].count_documents({})\n",
    "unable_scrap = 0\n",
    "scrap_data_count = status_count\n",
    "number = status_count + start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbed131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.ratemyprofessors.com/search/teachers?query=*&sid='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199009d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "Completed: 0.0 %\n",
      "Data Scraped:  0\n",
      "Unable to scrap count:  0\n",
      "URL:  https://www.ratemyprofessors.com/search/teachers?query=*&sid=1\n",
      "professor_count:  493\n"
     ]
    }
   ],
   "source": [
    "# while number < 24:\n",
    "while number < end:\n",
    "    print('\\n',number)\n",
    "    perc = scrap_data_count/(end-start) * 100\n",
    "    print(\"Completed: {} %\".format(perc))\n",
    "    print(\"Data Scraped: \", scrap_data_count)\n",
    "    print(\"Unable to scrap count: \", unable_scrap)\n",
    "    url = main_url + str(number+1)\n",
    "    get_url(url)\n",
    "    print('URL: ', url)\n",
    "    close_cookies_or_ad()\n",
    "    professor_count, uni_name = get_professor_count()\n",
    "    print('professor_count: ', professor_count)\n",
    "    for i in range(int(professor_count/8)):\n",
    "        try:\n",
    "            close_cookies_or_ad()\n",
    "            time.sleep(3)\n",
    "            show_more_btn()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            pass\n",
    "    if professor_count !=0:\n",
    "        close_cookies_or_ad()\n",
    "        data_result_page = get_page_source()\n",
    "        data = scrapper(data_result_page, uni_name, number+1)\n",
    "        db['rate_my_professir_v2'].insert_one(data)\n",
    "        db['status_lalit'].insert_one({\n",
    "            'institution_name': uni_name,\n",
    "            'url': url,\n",
    "            'information_retereaved': True,\n",
    "        })\n",
    "        print('Successfully Scrapped!!...')\n",
    "    elif professor_count == 0:\n",
    "        db['rate_my_professir_v2'].insert_one({\n",
    "            'uni_id': number+1,\n",
    "            'university_name': uni_name,\n",
    "            'professors': [],\n",
    "        })\n",
    "        db['status_lalit'].insert_one({\n",
    "            'institution_name': uni_name,\n",
    "            'url': url,\n",
    "            'information_retereaved': True,\n",
    "        })\n",
    "        print('Successfully Scrapped!!...')\n",
    "        \n",
    "    else:\n",
    "        db['status_lalit'].insert_one({\n",
    "            'institution_name': uni_name,\n",
    "            'url': url,\n",
    "            'information_retereaved': False,\n",
    "        })\n",
    "        print('Something went Wrong!')\n",
    "        print('Unable to scrap...!')\n",
    "    number = number + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b01ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
